{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87199f60",
   "metadata": {},
   "source": [
    "## Capitol Trades\n",
    "\n",
    "Scrape trading data from capitoltrades.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59baf89",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3df8d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Dict\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse, parse_qs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9163121",
   "metadata": {},
   "source": [
    "### Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c8a6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TradeRecord:\n",
    "    \"\"\"Data class representing a single trade record\"\"\"\n",
    "    politician: str\n",
    "    traded_issuer: str\n",
    "    published: str\n",
    "    traded: str\n",
    "    filed_after: str\n",
    "    owner: str\n",
    "    type: str\n",
    "    size: str\n",
    "    price: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6edf24",
   "metadata": {},
   "source": [
    "### Capitol Trades Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f5e803",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapitolTradesScraper:\n",
    "    \"\"\"Web scraper for Capitol Trades website using BeautifulSoup\"\"\"\n",
    "    \n",
    "    def __init__(self, delay: float = 2.0, scraper_api_key: str = None):\n",
    "        \"\"\"\n",
    "        Initialise the scraper with requests session\n",
    "        \n",
    "        Args:\n",
    "            delay: Delay between requests in seconds\n",
    "            scraper_api_key: ScraperAPI key for proxy rotation (optional)\n",
    "        \"\"\"\n",
    "        self.delay = delay\n",
    "        self.base_url = \"https://www.capitoltrades.com/trades\"\n",
    "        self.scraper_api_key = scraper_api_key\n",
    "        self.scraper_api_base = \"https://api.scraperapi.com\"\n",
    "        self.session = self._setup_session()\n",
    "        self.current_page = 1\n",
    "    \n",
    "    def _setup_session(self) -> requests.Session:\n",
    "        \"\"\"Configure and return requests session with proper headers\"\"\"\n",
    "        session = requests.Session()\n",
    "        \n",
    "        if not self.scraper_api_key:\n",
    "            session.headers.update({\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "                'Accept-Language': 'en-GB,en;q=0.5',\n",
    "                'Accept-Encoding': 'gzip, deflate',\n",
    "                'Connection': 'keep-alive',\n",
    "                'Upgrade-Insecure-Requests': '1'\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Using ScraperAPI with key ending in ...{self.scraper_api_key[-4:]}\")\n",
    "            \n",
    "        return session\n",
    "    \n",
    "    def _build_scraper_api_url(self, target_url: str) -> str:\n",
    "        \"\"\"Build ScraperAPI URL with parameters\"\"\"\n",
    "        if not self.scraper_api_key:\n",
    "            return target_url\n",
    "            \n",
    "        params = {\n",
    "            'api_key': self.scraper_api_key,\n",
    "            'url': target_url,\n",
    "            'render': 'false',\n",
    "            'country_code': 'gb'\n",
    "        }\n",
    "        \n",
    "        param_string = '&'.join([f\"{k}={requests.utils.quote(str(v))}\" for k, v in params.items()])\n",
    "        return f\"{self.scraper_api_base}?{param_string}\"\n",
    "    \n",
    "    def fetch_page(self, url: str = None) -> Optional[BeautifulSoup]:\n",
    "        \"\"\"Fetch and parse a webpage\"\"\"\n",
    "        target_url = url or self.base_url\n",
    "        request_url = self._build_scraper_api_url(target_url)\n",
    "        \n",
    "        try:\n",
    "            if self.scraper_api_key:\n",
    "                print(f\"Fetching via ScraperAPI: {target_url}\")\n",
    "            else:\n",
    "                print(f\"Fetching directly: {target_url}\")\n",
    "                \n",
    "            response = self.session.get(request_url, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            print(f\"Successfully fetched page {self.current_page}\")\n",
    "            return soup\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching page: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def scrape_page_data(self, soup: BeautifulSoup) -> List[TradeRecord]:\n",
    "        \"\"\"Extract trade data from a parsed webpage\"\"\"\n",
    "        trades = []\n",
    "        \n",
    "        tbody = soup.find('tbody')\n",
    "        if not tbody:\n",
    "            print(\"No table body found\")\n",
    "            return trades\n",
    "        \n",
    "        rows = tbody.find_all('tr')\n",
    "        if not rows:\n",
    "            print(\"No rows found in tbody\")\n",
    "            return trades\n",
    "            \n",
    "        print(f\"Processing {len(rows)} rows...\")\n",
    "        \n",
    "        for row_index, row in enumerate(rows):\n",
    "            cells = row.find_all('td')\n",
    "            if len(cells) >= 8:  # Must have at least 8 data columns\n",
    "                try:\n",
    "                    trade = self._extract_trade_from_row(row, cells, row_index)\n",
    "                    if trade:\n",
    "                        trades.append(trade)\n",
    "                        if row_index < 3:  # Show progress for first few trades\n",
    "                            print(f\"âœ“ Extracted: {trade.politician} - {trade.traded_issuer}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting trade from row {row_index}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        print(f\"Successfully extracted {len(trades)} trades from page {self.current_page}\")\n",
    "        return trades\n",
    "    \n",
    "    def _extract_trade_from_row(self, row, cells, row_index: int) -> Optional[TradeRecord]:\n",
    "        \"\"\"Extract trade data from a table row\"\"\"\n",
    "        # Extract politician name\n",
    "        politician_cell = cells[0]\n",
    "        politician_link = politician_cell.find('a', class_='text-txt-interactive')\n",
    "        politician = politician_link.get_text(strip=True) if politician_link else self._fallback_text_extraction(politician_cell, \"politician\")\n",
    "        \n",
    "        # Extract issuer name\n",
    "        issuer_cell = cells[1]\n",
    "        issuer_link = issuer_cell.find('a', class_='hover:no-underline')\n",
    "        if not issuer_link:\n",
    "            issuer_link = issuer_cell.find('a')  # Fallback to any link\n",
    "        issuer = issuer_link.get_text(strip=True) if issuer_link else self._fallback_text_extraction(issuer_cell, \"issuer\")\n",
    "        \n",
    "        # Extract published date\n",
    "        published = self._extract_date_from_cell(cells[2])\n",
    "        \n",
    "        # Extract traded date\n",
    "        traded = self._extract_date_from_cell(cells[3])\n",
    "        \n",
    "        # Extract filed after days\n",
    "        filed_after_span = cells[4].find('span', class_=re.compile('reporting-gap'))\n",
    "        if not filed_after_span:\n",
    "            # Look for any number in the cell\n",
    "            filed_after_text = cells[4].get_text(strip=True)\n",
    "            numbers = re.findall(r'\\d+', filed_after_text)\n",
    "            filed_after = numbers[0] if numbers else filed_after_text\n",
    "        else:\n",
    "            filed_after = filed_after_span.get_text(strip=True)\n",
    "        \n",
    "        # Extract owner\n",
    "        owner_span = cells[5].find('span', class_='q-label')\n",
    "        owner = owner_span.get_text(strip=True) if owner_span else self._fallback_text_extraction(cells[5], \"owner\")\n",
    "        \n",
    "        # Extract transaction type\n",
    "        type_span = cells[6].find('span', class_=re.compile('tx-type'))\n",
    "        trade_type = type_span.get_text(strip=True) if type_span else self._fallback_text_extraction(cells[6], \"type\")\n",
    "        \n",
    "        # Extract size\n",
    "        size_span = cells[7].find('span', class_='mt-1')\n",
    "        if not size_span:\n",
    "            size_span = cells[7].find('span')  # Fallback to any span\n",
    "        size = size_span.get_text(strip=True) if size_span else self._fallback_text_extraction(cells[7], \"size\")\n",
    "        \n",
    "        # Extract price (9th column if exists)\n",
    "        if len(cells) > 8:\n",
    "            price_span = cells[8].find('span')\n",
    "            price = price_span.get_text(strip=True) if price_span else self._fallback_text_extraction(cells[8], \"price\")\n",
    "        else:\n",
    "            price = \"N/A\"\n",
    "        \n",
    "        # Validate required fields\n",
    "        if politician == \"Unknown\" or issuer == \"Unknown\":\n",
    "            return None\n",
    "        \n",
    "        return TradeRecord(\n",
    "            politician=politician,\n",
    "            traded_issuer=issuer,\n",
    "            published=published,\n",
    "            traded=traded,\n",
    "            filed_after=filed_after,\n",
    "            owner=owner,\n",
    "            type=trade_type,\n",
    "            size=size,\n",
    "            price=price\n",
    "        )\n",
    "    \n",
    "    def _extract_date_from_cell(self, cell) -> str:\n",
    "        \"\"\"Extract date from a cell with date structure\"\"\"\n",
    "        date_div = cell.find('div', class_='text-center')\n",
    "        if date_div:\n",
    "            day_month_div = date_div.find('div', class_='text-size-3')\n",
    "            year_div = date_div.find('div', class_='text-size-2')\n",
    "            \n",
    "            if day_month_div and year_div:\n",
    "                day_month = day_month_div.get_text(strip=True)\n",
    "                year = year_div.get_text(strip=True)\n",
    "                return f\"{day_month} {year}\"\n",
    "        \n",
    "        # Fallback to cell text\n",
    "        return self._fallback_text_extraction(cell, \"date\")\n",
    "    \n",
    "    def _fallback_text_extraction(self, cell, field_name: str) -> str:\n",
    "        \"\"\"Fallback method to extract text from cell\"\"\"\n",
    "        text = cell.get_text(strip=True)\n",
    "        if not text:\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        # Clean up the text (remove extra whitespace)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # For politician names, try to extract just the name part\n",
    "        if field_name == \"politician\":\n",
    "            # Look for pattern: \"Name Party Chamber State\"\n",
    "            parts = text.split()\n",
    "            if len(parts) >= 2:\n",
    "                # Try to find where the name ends (before political info)\n",
    "                political_terms = ['Republican', 'Democrat', 'House', 'Senate']\n",
    "                name_parts = []\n",
    "                for part in parts:\n",
    "                    if part in political_terms:\n",
    "                        break\n",
    "                    name_parts.append(part)\n",
    "                \n",
    "                if name_parts:\n",
    "                    return ' '.join(name_parts)\n",
    "        \n",
    "        # Limit length for readability\n",
    "        return text[:100] if len(text) > 100 else text\n",
    "    \n",
    "    def find_next_page_url(self, soup: BeautifulSoup) -> Optional[str]:\n",
    "        \"\"\"Find the URL for the next page\"\"\"\n",
    "        # Look for pagination area\n",
    "        pagination_area = soup.find('div', class_=re.compile(r'relative.*flex.*items-center'))\n",
    "        \n",
    "        if pagination_area:\n",
    "            # Look for \"Go to next page\" link\n",
    "            next_link = pagination_area.find('a', {'aria-label': re.compile(r'Go to next page', re.IGNORECASE)})\n",
    "            if next_link:\n",
    "                href = next_link.get('href')\n",
    "                if href:\n",
    "                    return urljoin(self.base_url, href)\n",
    "        \n",
    "        # Fallback: Look for any page links\n",
    "        all_page_links = soup.find_all('a', href=re.compile(r'page=\\d+'))\n",
    "        for link in all_page_links:\n",
    "            href = link.get('href')\n",
    "            page_match = re.search(r'page=(\\d+)', href)\n",
    "            if page_match and int(page_match.group(1)) == self.current_page + 1:\n",
    "                return urljoin(self.base_url, href)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def scrape_all_pages(self, max_pages: int = None) -> List[TradeRecord]:\n",
    "        \"\"\"Scrape trade data from all available pages\"\"\"\n",
    "        all_trades = []\n",
    "        current_url = self.base_url\n",
    "        \n",
    "        while current_url and (max_pages is None or self.current_page <= max_pages):\n",
    "            print(f\"\\nScraping page {self.current_page}\")\n",
    "            \n",
    "            soup = self.fetch_page(current_url)\n",
    "            if not soup:\n",
    "                print(f\"Failed to fetch page {self.current_page}\")\n",
    "                break\n",
    "            \n",
    "            page_trades = self.scrape_page_data(soup)\n",
    "            \n",
    "            if not page_trades and self.current_page > 1:\n",
    "                print(\"No trades found - might be end of data\")\n",
    "                break\n",
    "                \n",
    "            all_trades.extend(page_trades)\n",
    "            \n",
    "            if max_pages and self.current_page >= max_pages:\n",
    "                print(f\"Reached maximum pages limit: {max_pages}\")\n",
    "                break\n",
    "            \n",
    "            next_url = self.find_next_page_url(soup)\n",
    "            if not next_url:\n",
    "                print(\"No more pages found\")\n",
    "                break\n",
    "            \n",
    "            if next_url == current_url:\n",
    "                print(\"Next URL same as current - stopping to avoid loop\")\n",
    "                break\n",
    "            \n",
    "            current_url = next_url\n",
    "            self.current_page += 1\n",
    "            \n",
    "            print(f\"Waiting {self.delay} seconds...\")\n",
    "            time.sleep(self.delay)\n",
    "        \n",
    "        print(f\"\\nğŸ‰ Scraping complete! Total trades: {len(all_trades)} from {self.current_page} page(s)\")\n",
    "        return all_trades\n",
    "    \n",
    "    def save_to_csv(self, trades: List[TradeRecord], filename: str = \"capitol_trades.csv\") -> None:\n",
    "        \"\"\"Save trades to CSV file with auto-generated ID column\"\"\"\n",
    "        if not trades:\n",
    "            print(\"No trades to save\")\n",
    "            return\n",
    "        \n",
    "        df = pd.DataFrame([trade.__dict__ for trade in trades])\n",
    "        df.insert(0, 'id', range(1, len(df) + 1))\n",
    "        \n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"ğŸ’¾ Saved {len(trades)} trades to {filename}\")\n",
    "    \n",
    "    def get_trade_summary(self, trades: List[TradeRecord]) -> Dict:\n",
    "        \"\"\"Generate summary statistics for scraped trades\"\"\"\n",
    "        if not trades:\n",
    "            return {}\n",
    "        \n",
    "        df = pd.DataFrame([trade.__dict__ for trade in trades])\n",
    "        \n",
    "        return {\n",
    "            'total_trades': len(trades),\n",
    "            'unique_politicians': df['politician'].nunique(),\n",
    "            'unique_issuers': df['traded_issuer'].nunique(),\n",
    "            'trade_types': df['type'].value_counts().to_dict(),\n",
    "            'owner_breakdown': df['owner'].value_counts().to_dict(),\n",
    "            'top_politicians': df['politician'].value_counts().head(5).to_dict(),\n",
    "            'top_issuers': df['traded_issuer'].value_counts().head(5).to_dict()\n",
    "        }\n",
    "    \n",
    "    def close(self) -> None:\n",
    "        \"\"\"Close the requests session\"\"\"\n",
    "        if self.session:\n",
    "            self.session.close()\n",
    "    \n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60c98cd",
   "metadata": {},
   "source": [
    "### Scrape Capitol Trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b82c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"ğŸš€ Starting Capitol Trades scraper...\")\n",
    "    \n",
    "    # Configuration\n",
    "    MAX_PAGES = 5  # Adjust as needed\n",
    "    DELAY_SECONDS = 3.0  # Be respectful to the server\n",
    "    \n",
    "    # Option 1: Use ScraperAPI (recommended for production)\n",
    "    # scraper_api_key = \"YOUR_SCRAPERAPI_KEY_HERE\"\n",
    "    # with CapitolTradesScraper(delay=DELAY_SECONDS, scraper_api_key=scraper_api_key) as scraper:\n",
    "    \n",
    "    # Option 2: Direct scraping (for testing)\n",
    "    with CapitolTradesScraper(delay=DELAY_SECONDS) as scraper:\n",
    "        trades = scraper.scrape_all_pages(max_pages=MAX_PAGES)\n",
    "        \n",
    "        if trades:\n",
    "            # Save to CSV\n",
    "            scraper.save_to_csv(trades, \"capitol_trades.csv\")\n",
    "            \n",
    "            # Display summary\n",
    "            summary = scraper.get_trade_summary(trades)\n",
    "            print(f\"\\nğŸ“Š SCRAPING SUMMARY:\")\n",
    "            print(f\"ğŸ“ˆ Total trades: {summary.get('total_trades', 0)}\")\n",
    "            print(f\"ğŸ‘¥ Unique politicians: {summary.get('unique_politicians', 0)}\")\n",
    "            print(f\"ğŸ¢ Unique issuers: {summary.get('unique_issuers', 0)}\")\n",
    "            \n",
    "            trade_types = summary.get('trade_types', {})\n",
    "            if trade_types:\n",
    "                print(f\"\\nğŸ’¹ Trade Types:\")\n",
    "                for trade_type, count in trade_types.items():\n",
    "                    print(f\"   {trade_type}: {count}\")\n",
    "            \n",
    "            owner_breakdown = summary.get('owner_breakdown', {})\n",
    "            if owner_breakdown:\n",
    "                print(f\"\\nğŸ‘¤ Owner Breakdown:\")\n",
    "                for owner, count in owner_breakdown.items():\n",
    "                    print(f\"   {owner}: {count}\")\n",
    "            \n",
    "            top_politicians = summary.get('top_politicians', {})\n",
    "            if top_politicians:\n",
    "                print(f\"\\nğŸ›ï¸ Top Politicians by Trade Count:\")\n",
    "                for politician, count in list(top_politicians.items())[:5]:\n",
    "                    print(f\"   {politician}: {count} trades\")\n",
    "            \n",
    "            top_issuers = summary.get('top_issuers', {})\n",
    "            if top_issuers:\n",
    "                print(f\"\\nğŸ¢ Top Issuers by Trade Count:\")\n",
    "                for issuer, count in list(top_issuers.items())[:5]:\n",
    "                    print(f\"   {issuer}: {count} trades\")\n",
    "            \n",
    "            # Show sample trades\n",
    "            print(f\"\\nğŸ“‹ Sample Trades:\")\n",
    "            for i, trade in enumerate(trades[:3], 1):\n",
    "                print(f\"\\nTrade {i}:\")\n",
    "                print(f\"   Politician: {trade.politician}\")\n",
    "                print(f\"   Issuer: {trade.traded_issuer}\")\n",
    "                print(f\"   Type: {trade.type}\")\n",
    "                print(f\"   Size: {trade.size}\")\n",
    "                print(f\"   Price: {trade.price}\")\n",
    "                print(f\"   Published: {trade.published}\")\n",
    "                print(f\"   Traded: {trade.traded}\")\n",
    "        else:\n",
    "            print(\"âŒ No trades were scraped. Check your internet connection or the website may have changed.\")\n",
    "\n",
    "    print(\"\\nâœ… Scraper finished!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
